{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/home/albertdm99/Uni/TFG/neural-admixture'\n",
    "data_path = f'{root}/data/all_chm_combined_snps_world_2M_with_labels.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _L0Norm(nn.Module):\n",
    "    def __init__(self, origin, loc_mean=0, loc_sdev=0.01, beta=2 / 3, gamma=-0.1,\n",
    "                 zeta=1.1, fix_temp=True):\n",
    "        \"\"\"\n",
    "        Base class of layers using L0 Norm\n",
    "        :param origin: original layer such as nn.Linear(..), nn.Conv2d(..)\n",
    "        :param loc_mean: mean of the normal distribution which generates initial location parameters\n",
    "        :param loc_sdev: standard deviation of the normal distribution which generates initial location parameters\n",
    "        :param beta: initial temperature parameter\n",
    "        :param gamma: lower bound of \"stretched\" s\n",
    "        :param zeta: upper bound of \"stretched\" s\n",
    "        :param fix_temp: True if temperature is fixed\n",
    "        \"\"\"\n",
    "        super(_L0Norm, self).__init__()\n",
    "        self._origin = origin\n",
    "        self._size = self._origin.weight.size()\n",
    "        self.loc = nn.Parameter(torch.zeros(self._size).normal_(loc_mean, loc_sdev))\n",
    "        self.temp = beta if fix_temp else nn.Parameter(torch.zeros(1).fill_(beta))\n",
    "        self.register_buffer(\"uniform\", torch.zeros(self._size))\n",
    "        self.gamma = gamma\n",
    "        self.zeta = zeta\n",
    "        self.gamma_zeta_ratio = math.log(-gamma / zeta)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def _hard_sigmoid(self, x):\n",
    "        return torch.min(torch.max(x, torch.zeros_like(x)), torch.ones_like(x))\n",
    "\n",
    "    def _get_mask(self):\n",
    "        if self.training:\n",
    "            self.uniform.uniform_()\n",
    "            u = Variable(self.uniform)\n",
    "            s = self.sigmoid((torch.log(u) - torch.log(1 - u) + self.loc) / self.temp)\n",
    "            s = s * (self.zeta - self.gamma) + self.gamma\n",
    "            penalty = self.sigmoid(self.loc - self.temp * self.gamma_zeta_ratio).sum()\n",
    "        else:\n",
    "            s = self.sigmoid(self.loc) * (self.zeta - self.gamma) + self.gamma\n",
    "            penalty = 0\n",
    "        return self._hard_sigmoid(s), penalty\n",
    "\n",
    "\n",
    "class L0Linear(_L0Norm):\n",
    "    def __init__(self, in_features, out_features, bias=True, **kwargs):\n",
    "        super(L0Linear, self).__init__(nn.Linear(in_features, out_features, bias=bias), **kwargs)\n",
    "\n",
    "    def forward(self, input):\n",
    "        mask, penalty = self._get_mask()\n",
    "        return F.linear(input, self._origin.weight * mask, self._origin.bias), penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConstrainedLinear(torch.nn.Module):\n",
    "    def __init__ (self, input_size, output_size, bias=True): \n",
    "        super().__init__() \n",
    "        self.W = nn.Parameter(torch.zeros(input_size, output_size)) \n",
    "        self.W = nn.init.kaiming_normal_(self.W)\n",
    "        self.bias = bias\n",
    "        if self.bias:\n",
    "            self.b = nn.Parameter(torch.ones(output_size)) \n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.bias:\n",
    "            return torch.addmm(self.b, x, torch.sigmoid(self.W))\n",
    "        return torch.mm(x, torch.sigmoid(self.W)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdmixtureAE(torch.nn.Module):\n",
    "    def __init__(self, k, num_features, beta_l0=2/3, gamma_l0=-0.1, zeta_l0=1.1, lambda_l0=0.1):\n",
    "        super().__init__()\n",
    "        self.k = k\n",
    "        self.num_features = num_features\n",
    "        self.beta_l0, self.gamma_l0, self.zeta_l0 = beta_l0, gamma_l0, zeta_l0\n",
    "        self.lambda_l0 = lambda_l0\n",
    "        self.encoder = L0Linear(self.num_features, self.k, bias=False, beta=self.beta_l0, gamma=self.gamma_l0, zeta=self.zeta_l0)\n",
    "        self.decoder = ConstrainedLinear(self.k, num_features, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        enc, l0_pen = self.encoder(X)\n",
    "        hid_state = self.softmax(enc)\n",
    "        reconstruction = self.decoder(hid_state)\n",
    "        return reconstruction, hid_state, l0_pen/X.shape[0]\n",
    "        \n",
    "    def train(self, trX, optimizer, loss_f, num_epochs, device, batch_size=0, valX=None, display_logs=True):\n",
    "        for ep in range(num_epochs):\n",
    "            if display_logs:\n",
    "                print(f'------------- EPOCH {ep+1} -------------')\n",
    "            tr_loss, val_loss = self._run_epoch(trX, optimizer, loss_f, batch_size, valX, device)\n",
    "            if display_logs:\n",
    "                print(f'Mean training loss: {tr_loss}')\n",
    "                if val_loss is not None:\n",
    "                    print(f'Mean validation loss: {val_loss}')\n",
    "        return tr_loss, val_loss\n",
    "\n",
    "    def _batch_generator(self, X, batch_size=0):\n",
    "        if batch_size < 1:\n",
    "            yield torch.tensor(X, dtype=torch.float32)\n",
    "        else:\n",
    "            for i in range(0, X.shape[0], batch_size):\n",
    "                yield torch.tensor(X[i:i+batch_size], dtype=torch.float32)\n",
    "\n",
    "    def _validate(self, valX, loss_f, batch_size, device):\n",
    "        acum_val_loss = 0\n",
    "        for X in self._batch_generator(valX, batch_size):\n",
    "            rec, _, _ = self.forward(X.to(device))\n",
    "            acum_val_loss += loss_f(rec, X).cpu().item()\n",
    "        return acum_val_loss\n",
    "\n",
    "        \n",
    "    def _run_step(self, X, optimizer, loss_f):\n",
    "        self.zero_grad()\n",
    "        rec, _, l0_pen = self.forward(X)\n",
    "        loss = loss_f(rec, X)+self.lambda_l0*l0_pen\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        return loss\n",
    "\n",
    "    def _run_epoch(self, trX, optimizer, loss_f, batch_size, valX, device):\n",
    "        tr_loss, val_loss = 0, None\n",
    "        for X in self._batch_generator(trX, batch_size):\n",
    "            step_loss = self._run_step(X.to(device), optimizer, loss_f)\n",
    "            tr_loss += step_loss.cpu().item()\n",
    "        if valX is not None:\n",
    "            val_loss = self._validate(valX, loss_f, device)\n",
    "            return tr_loss / trX.shape[0], val_loss / valX.shape[0]\n",
    "        return tr_loss / trX.shape[0], None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npzfile = np.load(data_path, allow_pickle=True)\n",
    "snps = npzfile['snps']\n",
    "del npzfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AdmixtureAE(k=8, num_features=snps.shape[1], lambda_l0=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "optimizer = torch.optim.Adam(model.parameters(), learning_rate)\n",
    "loss_f = nn.MSELoss(reduction='sum')\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(snps, optimizer, loss_f, num_epochs, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
